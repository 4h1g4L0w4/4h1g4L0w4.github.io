---
title: "Compresi√≥n de Contexto en RAG: Optimizando Tokens sin Perder Calidad"
date: 2025-11-05
tags: [proyectos, trabajo]
categories: [Proyectos, Trabajo]
author: augusto
description: "Exploramos tres estrategias de compresi√≥n de contexto para sistemas RAG, comparando eficiencia, calidad y costo en tokens. Implementaci√≥n pr√°ctica con Python, Qdrant y Jan AI."
mermaid: true
---

## Introducci√≥n: El Problema del Contexto en RAG

Los sistemas de **RAG (Retrieval Augmented Generation)** han revolucionado la forma en que los modelos de lenguaje acceden a informaci√≥n externa. Sin embargo, un desaf√≠o cr√≠tico surge cuando recuperamos demasiada informaci√≥n de la base de datos vectorial: **el consumo excesivo de tokens**.

En este art√≠culo, exploramos la **compresi√≥n de contexto** como soluci√≥n a este problema, implementando y comparando tres estrategias diferentes: **Summarize (Abstractive)**, **Extractive Top-K**, y **Rerank + Truncate**.

## ¬øPor Qu√© Necesitamos Comprimir el Contexto?

### El Problema Fundamental

En un sistema RAG t√≠pico, el flujo es el siguiente:

```mermaid
graph LR
    A[Consulta del Usuario] --> B[Embedding de Consulta]
    B --> C[B√∫squeda Vectorial<br/>en Base de Datos]
    C --> D[Recuperaci√≥n de Chunks<br/>Relevantes]
    D --> E[Contexto Completo<br/>2000-5000 tokens]
    E --> F[LLM Genera Respuesta]
    
    style E fill:#ffcccc
    style F fill:#ccffcc
```

**El problema**: Cuando recuperamos m√∫ltiples chunks de documentos (por ejemplo, 5 chunks de 500 palabras cada uno), podemos f√°cilmente acumular 2,000-5,000 tokens de contexto. Esto tiene varios problemas:

1. **Costo Econ√≥mico**: Los modelos de lenguaje cobran por token. M√°s tokens = mayor costo.
2. **L√≠mites de Contexto**: Muchos modelos tienen l√≠mites de ventana de contexto (4K, 8K, 32K tokens).
3. **Ruido Sem√°ntico**: Informaci√≥n irrelevante puede confundir al modelo.
4. **Latencia**: Procesar m√°s tokens requiere m√°s tiempo de inferencia.

### La Soluci√≥n: Compresi√≥n de Contexto

La compresi√≥n de contexto es el proceso de **reducir la cantidad de tokens del contexto recuperado manteniendo la informaci√≥n m√°s relevante para responder la consulta**. No es simplemente truncar texto, sino **inteligentemente seleccionar o resumir** lo esencial.

```mermaid
graph LR
    A[Contexto Original<br/>5000 tokens] --> B[Compresor]
    B --> C1[Summarize<br/>800 tokens]
    B --> C2[Extractive Top-K<br/>500 tokens]
    B --> C3[Rerank + Truncate<br/>800 tokens]
    
    C1 --> D[LLM Genera Respuesta]
    C2 --> D
    C3 --> D
    
    style A fill:#ffcccc
    style C1 fill:#ccffcc
    style C2 fill:#ccffcc
    style C3 fill:#ccffcc
```

## Arquitectura del Sistema Implementado

Antes de profundizar en las estrategias, veamos la arquitectura completa del sistema que implementamos:

```mermaid
graph TB
    subgraph "Ingesta de Documentos"
        PDF[PDFs] --> LOADER[PDF Loader<br/>Extracci√≥n de Texto]
        LOADER --> CHUNKER[Chunker<br/>Divisi√≥n en Chunks]
        CHUNKER --> EMB1[Embedder<br/>BAAI/bge-m3]
        EMB1 --> QDRANT[(Qdrant<br/>Vector DB)]
    end
    
    subgraph "Sistema RAG con Compresi√≥n"
        QUERY[Consulta Usuario] --> EMB2[Embedder<br/>Genera Query Vector]
        EMB2 --> QDRANT
        QDRANT --> RETRIEVE[Recuperaci√≥n<br/>Top-K Chunks]
        RETRIEVE --> COMP[Context Compressor<br/>3 Estrategias]
        
        COMP --> S1[Summarize]
        COMP --> S2[Extractive Top-K]
        COMP --> S3[Rerank + Truncate]
        
        S1 --> LLM[Jan AI<br/>Genera Respuestas]
        S2 --> LLM
        S3 --> LLM
    end
    
    LLM --> OUTPUT[Respuestas Comparadas<br/>+ M√©tricas de Tokens]
    
    style QDRANT fill:#e1f5ff
    style COMP fill:#fff9c4
    style LLM fill:#fff4e1
```

**Componentes clave**:
- **Qdrant**: Base de datos vectorial para almacenar embeddings
- **BAAI/bge-m3**: Modelo de embeddings de alta calidad (1024 dimensiones)
- **Jan AI**: Servidor local de modelos de lenguaje
- **Compresor de Contexto**: M√≥dulo que implementa las 3 estrategias

## Estrategia 1: Summarize (Compresi√≥n Abstractiva)

### Fundamentaci√≥n Te√≥rica

La compresi√≥n **abstractiva** utiliza un modelo de lenguaje para **generar un resumen nuevo** del contexto, reescribiendo y condensando la informaci√≥n. Esta es una estrategia **generativa** que crea texto nuevo, no solo selecciona texto existente.

### ¬øPor Qu√© Funciona?

1. **Compresi√≥n Alta**: Puede reducir contextos de 5000 tokens a 800 tokens manteniendo informaci√≥n clave.
2. **Coherencia**: El resumen es coherente y fluido, no fragmentos desconectados.
3. **Enfoque en la Consulta**: El prompt gu√≠a al modelo a enfocarse en informaci√≥n relevante para responder.

### Implementaci√≥n

```python
def compress_context(self, query: str, context: str) -> tuple[str, int]:
    """Comprime el contexto usando Jan AI (resumen guiado por la consulta)"""
    
    system_prompt = (
        "Eres un compresor de contexto. Dado el contexto y la consulta, "
        "devuelve un resumen conciso con la informaci√≥n m√°s relevante "
        "para responder la consulta. "
        f"Limita el resumen objetivo a ~{self.compressor_target_tokens} tokens. "
        "Mant√©n citas de fuente (archivo y p√°gina) si se incluyen entre corchetes."
    )
    
    user_prompt = (
        f"Consulta: {query}\n\n"
        f"Contexto:\n{context}\n\n"
        "Resumen (conciso y espec√≠fico a la consulta):"
    )
    
    # Llamada a Jan AI para generar resumen
    response = requests.post(
        f"{self.jan_ai_url}/v1/chat/completions",
        json={
            "model": self.jan_ai_chat_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.2,  # Baja temperatura para res√∫menes consistentes
        }
    )
    
    summary = response.json()["choices"][0]["message"]["content"]
    return summary, self.count_tokens(summary)
```

### Flujo del Proceso

```mermaid
sequenceDiagram
    participant RAG as Sistema RAG
    participant QD as Qdrant
    participant COMP as Compresor Summarize
    participant LLM as Jan AI
    participant USER as Usuario

    USER->>RAG: Pregunta
    RAG->>QD: B√∫squeda vectorial
    QD-->>RAG: Contexto (5000 tokens)
    
    RAG->>COMP: Contexto + Consulta
    COMP->>LLM: Generar resumen guiado
    LLM-->>COMP: Resumen (800 tokens)
    COMP-->>RAG: Contexto comprimido
    
    RAG->>LLM: Prompt con contexto comprimido
    LLM-->>RAG: Respuesta final
    RAG-->>USER: Respuesta + M√©tricas
```

### Ventajas y Desventajas

**Ventajas**:
- ‚úÖ **Alta compresi√≥n**: Reducci√≥n de 70-85% de tokens
- ‚úÖ **Coherencia**: Texto fluido y coherente
- ‚úÖ **Adaptabilidad**: El modelo decide qu√© informaci√≥n es m√°s relevante
- ‚úÖ **Preserva relaciones**: Mantiene conexiones sem√°nticas entre conceptos

**Desventajas**:
- ‚ùå **Costo adicional**: Requiere una llamada extra al LLM
- ‚ùå **Latencia**: M√°s lento que estrategias extractivas
- ‚ùå **P√©rdida de detalles**: Puede omitir informaci√≥n espec√≠fica importante
- ‚ùå **Alucinaciones**: Riesgo de introducir informaci√≥n no presente en el original

### Cu√°ndo Usar

Ideal para:
- Contextos muy largos (>3000 tokens)
- Cuando la coherencia es m√°s importante que preservar texto exacto
- Aplicaciones donde el costo de tokens es cr√≠tico
- Consultas que requieren s√≠ntesis de m√∫ltiples conceptos

## Estrategia 2: Extractive Top-K

### Fundamentaci√≥n Te√≥rica

La compresi√≥n **extractiva** selecciona las **oraciones m√°s relevantes** del contexto original sin modificarlas. Utiliza **similitud sem√°ntica** (embeddings) para rankear oraciones seg√∫n su relevancia a la consulta.

### ¬øPor Qu√© Funciona?

1. **Velocidad**: No requiere llamadas a LLM, solo c√°lculos de similitud coseno.
2. **Preservaci√≥n**: Mantiene el texto original sin modificaciones.
3. **Determinismo**: Siempre produce el mismo resultado para la misma entrada.
4. **Transparencia**: El usuario puede ver exactamente qu√© texto se us√≥.

### Implementaci√≥n

```python
def compress_extractive_topk(self, query: str, context: str) -> tuple[str, int]:
    """Selecciona las K oraciones m√°s relevantes usando similitud coseno"""
    
    # 1. Dividir contexto en oraciones
    sentences = self._split_sentences(context)
    
    # 2. Generar embeddings de consulta y oraciones
    query_vec = self.embedder.embed_documents([query])[0]
    sent_vecs = self.embedder.embed_documents(sentences)
    
    # 3. Calcular similitud coseno (embeddings ya est√°n normalizados)
    import numpy as np
    q = np.array(query_vec)
    similarities = [
        float(np.dot(q, np.array(v))) 
        for v in sent_vecs
    ]
    
    # 4. Rankear y seleccionar Top-K
    ranked = sorted(
        zip(sentences, similarities), 
        key=lambda x: x[1], 
        reverse=True
    )
    top_sentences = [sentence for sentence, _ in ranked[:self.compressor_topk]]
    
    # 5. Concatenar oraciones seleccionadas
    summary = "\n".join(top_sentences)
    return summary, self.count_tokens(summary)
```

### Flujo del Proceso

```mermaid
graph TD
    A[Contexto Original<br/>50 oraciones] --> B[Split en Oraciones]
    B --> C[Generar Embeddings<br/>Query + Oraciones]
    C --> D[Calcular Similitud Coseno]
    D --> E[Ranking por Similitud]
    E --> F[Top-K Oraciones<br/>K=5]
    F --> G[Contexto Comprimido<br/>5 oraciones]
    
    style D fill:#e1bee7
    style F fill:#c8e6c9
```

### An√°lisis Matem√°tico

La similitud coseno entre dos vectores normalizados se calcula como:

$$
\text{similitud}(q, s) = \frac{q \cdot s}{||q|| \cdot ||s||} = q \cdot s
$$

Donde:
- $q$: embedding de la consulta (normalizado)
- $s$: embedding de la oraci√≥n (normalizado)
- Como ambos est√°n normalizados, el producto punto es igual a la similitud coseno

**Complejidad**: O(n √ó d) donde n es el n√∫mero de oraciones y d es la dimensi√≥n del embedding (1024 para bge-m3).

### Ventajas y Desventajas

**Ventajas**:
- ‚úÖ **Muy r√°pido**: Solo requiere c√°lculos de similitud vectorial
- ‚úÖ **Preserva texto original**: No hay riesgo de alucinaciones
- ‚úÖ **Determin√≠stico**: Resultados reproducibles
- ‚úÖ **Bajo costo**: No requiere llamadas adicionales a LLM

**Desventajas**:
- ‚ùå **P√©rdida de contexto**: Puede romper el flujo entre oraciones
- ‚ùå **Fragmentaci√≥n**: Las oraciones pueden estar desconectadas
- ‚ùå **Sin reescritura**: No puede condensar informaci√≥n
- ‚ùå **Dependencia de embeddings**: La calidad depende de la calidad del modelo de embeddings

### Cu√°ndo Usar

Ideal para:
- Aplicaciones donde la velocidad es cr√≠tica
- Cuando necesitas preservar texto exacto (citas, n√∫meros, fechas)
- Sistemas con presupuesto limitado de API
- Contextos donde la informaci√≥n est√° bien distribuida en oraciones individuales

## Estrategia 3: Rerank + Truncate

### Fundamentaci√≥n Te√≥rica

Esta estrategia combina **re-ranking** de chunks por relevancia con **truncamiento inteligente** hasta un presupuesto de tokens. A diferencia de Extractive Top-K que trabaja a nivel de oraciones, Rerank + Truncate opera a **nivel de chunks completos**.

### ¬øPor Qu√© Funciona?

1. **Preserva chunks completos**: Mantiene la integridad de cada chunk
2. **Control preciso**: Respeta exactamente el presupuesto de tokens
3. **Re-ranking**: Los chunks m√°s relevantes se priorizan
4. **Balance**: Combina velocidad (extractiva) con control (presupuesto)

### Implementaci√≥n

```python
def compress_rerank_truncate(
    self, 
    query: str, 
    docs: List[Dict[str, Any]]
) -> tuple[str, int]:
    """Reordena chunks por relevancia y concatena hasta presupuesto"""
    
    # 1. Generar embedding de consulta
    query_vec = self.embedder.embed_documents([query])[0]
    
    # 2. Generar embeddings de cada chunk
    texts = [doc.get("text", "") for doc in docs]
    chunk_vecs = self.embedder.embed_documents(texts)
    
    # 3. Calcular similitud y rankear
    import numpy as np
    q = np.array(query_vec)
    similarities = [
        float(np.dot(q, np.array(v))) 
        for v in chunk_vecs
    ]
    ranked_indices = sorted(
        range(len(texts)), 
        key=lambda i: similarities[i], 
        reverse=True
    )
    
    # 4. Concatena chunks hasta presupuesto de tokens
    budget = self.rerank_budget_tokens
    selected_chunks = []
    used_tokens = 0
    
    for idx in ranked_indices:
        chunk = texts[idx]
        chunk_tokens = self.count_tokens(chunk)
        
        if used_tokens + chunk_tokens > budget:
            break  # No cabe m√°s
            
        selected_chunks.append(chunk)
        used_tokens += chunk_tokens
    
    summary = "\n\n".join(selected_chunks)
    return summary, self.count_tokens(summary)
```

### Flujo del Proceso

```mermaid
graph TD
    A[Chunks de Qdrant<br/>5 chunks, 500 tokens c/u] --> B[Generar Embeddings<br/>Query + Chunks]
    B --> C[Calcular Similitud<br/>Coseno]
    C --> D[Ordenar por Relevancia]
    D --> E[Concatena hasta Budget<br/>800 tokens]
    E --> F[Chunk 1: 300 tokens]
    E --> G[Chunk 3: 250 tokens]
    E --> H[Chunk 2: 250 tokens]
    F --> I[Contexto Final<br/>800 tokens]
    G --> I
    H --> I
    
    style D fill:#ffccbc
    style E fill:#c8e6c9
```

### Ventajas y Desventajas

**Ventajas**:
- ‚úÖ **Preserva integridad**: Mantiene chunks completos
- ‚úÖ **Control preciso**: Respeta exactamente el presupuesto
- ‚úÖ **Balance**: Velocidad razonable con buena compresi√≥n
- ‚úÖ **Flexibilidad**: F√°cil ajustar el presupuesto seg√∫n necesidades

**Desventajas**:
- ‚ùå **Cortes abruptos**: Puede cortar chunks importantes
- ‚ùå **Sin reordenamiento interno**: No reordena dentro de chunks
- ‚ùå **Dependencia de chunking**: La calidad depende de c√≥mo se dividieron los documentos

### Cu√°ndo Usar

Ideal para:
- Cuando necesitas control preciso de tokens
- Aplicaciones donde los chunks son unidades sem√°nticas coherentes
- Sistemas que requieren balance entre velocidad y calidad
- Cuando el presupuesto de tokens es un hard constraint

## Comparaci√≥n de Estrategias

### M√©tricas de Comparaci√≥n

| Estrategia | Reducci√≥n Tokens | Velocidad | Coherencia | Preserva Texto | Costo LLM |
|------------|------------------|-----------|------------|----------------|-----------|
| **Directo BD** | 0% (baseline) | ‚ö°‚ö°‚ö° | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ | Bajo |
| **Summarize** | 70-85% | ‚ö° | ‚úÖ‚úÖ | ‚ùå | Alto (2 llamadas) |
| **Extractive Top-K** | 60-80% | ‚ö°‚ö°‚ö° | ‚úÖ | ‚úÖ‚úÖ‚úÖ | Bajo |
| **Rerank + Truncate** | 50-70% | ‚ö°‚ö° | ‚úÖ‚úÖ | ‚úÖ‚úÖ | Bajo |

### Visualizaci√≥n Comparativa

```mermaid
graph LR
    subgraph "Contexto Original"
        A[5000 tokens<br/>Informaci√≥n Completa]
    end
    
    subgraph "Estrategias de Compresi√≥n"
        B[Summarize<br/>800 tokens<br/>85% reducci√≥n]
        C[Extractive Top-K<br/>500 tokens<br/>90% reducci√≥n]
        D[Rerank + Truncate<br/>800 tokens<br/>84% reducci√≥n]
    end
    
    subgraph "Calidad de Respuesta"
        E[Alta Coherencia<br/>Baja Preservaci√≥n]
        F[Media Coherencia<br/>Alta Preservaci√≥n]
        G[Alta Coherencia<br/>Media Preservaci√≥n]
    end
    
    A --> B
    A --> C
    A --> D
    
    B --> E
    C --> F
    D --> G
    
    style A fill:#ffcccc
    style B fill:#c8e6c9
    style C fill:#c8e6c9
    style D fill:#c8e6c9
```

### Resultados Emp√≠ricos

En nuestro sistema de prueba con documentos de medicina del trabajo, observamos:

**Ejemplo Real: Consulta sobre RCP (Reanimaci√≥n Cardio Pulmonar)**

| M√©trica | Directo BD | Summarize | Extractive Top-K | Rerank + Truncate |
|---------|------------|-----------|------------------|-------------------|
| **Tokens Contexto** | 2,788 | 668 | 52 | 637 |
| **Reducci√≥n** | - | 76% | 98% | 77% |
| **Tiempo Procesamiento** | 2.3s | 8.5s | 0.8s | 1.2s |
| **Calidad Respuesta** | Excelente | Muy Buena | Buena | Muy Buena |
| **Preservaci√≥n Detalles** | 100% | ~70% | ~95% | ~85% |

**Observaciones clave**:
- **Extractive Top-K** logr√≥ la mayor reducci√≥n (98%) pero con menor coherencia
- **Summarize** mantuvo mejor calidad sem√°ntica pero requiere m√°s tiempo
- **Rerank + Truncate** ofreci√≥ el mejor balance costo-beneficio

## La Importancia de la Compresi√≥n de Contexto en RAG

### Impacto en Costos

Consideremos un sistema RAG que procesa 1000 consultas por d√≠a:

**Sin Compresi√≥n**:
- Promedio: 3000 tokens por contexto
- Costo: $0.03 por consulta (modelo GPT-4)
- **Costo diario: $30**

**Con Compresi√≥n (Summarize - 80% reducci√≥n)**:
- Promedio: 600 tokens por contexto
- Costo: $0.006 por consulta
- **Costo diario: $6**

**Ahorro: $24/d√≠a = $8,760/a√±o** üéâ

### Impacto en Rendimiento

```mermaid
graph TD
    A[Consulta Usuario] --> B{Contexto > 4000 tokens?}
    B -->|S√≠| C[Error: Context Window Exceeded]
    B -->|No| D[LLM Procesa]
    
    E[Consulta Usuario] --> F[Compresor]
    F --> G[Contexto < 1000 tokens]
    G --> H[LLM Procesa R√°pido]
    
    style C fill:#ffcccc
    style H fill:#ccffcc
```

### Impacto en Calidad

La compresi√≥n inteligente puede **mejorar** la calidad de las respuestas:

1. **Reduce Ruido**: Elimina informaci√≥n irrelevante que puede confundir al modelo
2. **Enfoque**: El modelo se concentra en informaci√≥n m√°s relevante
3. **Coherencia**: Contextos m√°s cortos y coherentes producen respuestas m√°s precisas

### Casos de Uso Reales

#### 1. Sistemas de Soporte T√©cnico

**Problema**: Base de conocimiento con 10,000+ documentos, cada consulta recupera m√∫ltiples art√≠culos.

**Soluci√≥n**: Usar **Rerank + Truncate** para mantener chunks completos de documentaci√≥n t√©cnica mientras respeta l√≠mites de tokens.

#### 2. Asistentes de Investigaci√≥n

**Problema**: Recuperaci√≥n de m√∫ltiples papers cient√≠ficos (contextos de 10,000+ tokens).

**Soluci√≥n**: Usar **Summarize** para crear res√∫menes coherentes que sinteticen informaci√≥n de m√∫ltiples fuentes.

#### 3. Chatbots Empresariales

**Problema**: Pol√≠ticas y procedimientos extensos, necesidad de velocidad y bajo costo.

**Soluci√≥n**: Usar **Extractive Top-K** para selecci√≥n r√°pida de secciones relevantes sin llamadas adicionales a LLM.

## Implementaci√≥n Pr√°ctica: Arquitectura del Sistema

### Arquitectura Completa

```mermaid
graph TB
    subgraph "Capa de Datos"
        PDF[Documentos PDF]
        QD[(Qdrant<br/>Vector Store)]
    end
    
    subgraph "Capa de Procesamiento"
        ING[Ingester<br/>PDF ‚Üí Chunks ‚Üí Embeddings]
        EMB[Embedder<br/>BAAI/bge-m3]
    end
    
    subgraph "Capa de RAG"
        RET[Retriever<br/>Vector Search]
        COMP[Context Compressor]
        LLM[Jan AI<br/>Chat Model]
    end
    
    subgraph "Estrategias de Compresi√≥n"
        S1[Summarize]
        S2[Extractive Top-K]
        S3[Rerank + Truncate]
    end
    
    PDF --> ING
    ING --> EMB
    EMB --> QD
    
    RET --> QD
    RET --> COMP
    COMP --> S1
    COMP --> S2
    COMP --> S3
    
    S1 --> LLM
    S2 --> LLM
    S3 --> LLM
    
    LLM --> OUTPUT[Respuestas Comparadas]
    
    style COMP fill:#fff9c4
    style LLM fill:#fff4e1
```

### Tecnolog√≠as Utilizadas

- **Qdrant**: Base de datos vectorial de alto rendimiento
- **BAAI/bge-m3**: Modelo de embeddings multiling√ºe (1024 dimensiones)
- **Jan AI**: Servidor local de modelos de lenguaje
- **Python**: FastAPI para API REST, Rich para UI terminal
- **Docker**: Contenedorizaci√≥n completa del sistema

## Conclusiones y Recomendaciones

### Conclusiones Principales

1. **La compresi√≥n de contexto es esencial** en sistemas RAG para controlar costos y mantener calidad.

2. **No hay una estrategia universal**: Cada estrategia tiene sus fortalezas seg√∫n el caso de uso.

3. **El balance es clave**: Encontrar el equilibrio entre reducci√≥n de tokens, velocidad y calidad.

4. **La comparaci√≥n es valiosa**: Implementar m√∫ltiples estrategias permite elegir la mejor seg√∫n contexto.

### Recomendaciones de Uso

**Para aplicaciones de alta velocidad y bajo costo**:
‚Üí Usa **Extractive Top-K**

**Para aplicaciones que requieren m√°xima compresi√≥n**:
‚Üí Usa **Summarize**

**Para aplicaciones que necesitan balance y control preciso**:
‚Üí Usa **Rerank + Truncate**

**Para comparaci√≥n y optimizaci√≥n**:
‚Üí Implementa las tres y mide m√©tricas en tu dominio espec√≠fico

### Pr√≥ximos Pasos

1. **Implementar m√©tricas de calidad**: BLEU, ROUGE, o evaluaci√≥n humana
2. **A/B Testing**: Comparar respuestas comprimidas vs. directas
3. **Estrategias h√≠bridas**: Combinar m√∫ltiples estrategias
4. **Compresi√≥n adaptativa**: Seleccionar estrategia seg√∫n caracter√≠sticas del contexto

## Referencias y Recursos

- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [BAAI/bge-m3 Paper](https://arxiv.org/abs/2402.03216)
- [Jan AI](https://jan.ai/)
- [RAG Survey Papers](https://arxiv.org/abs/2312.10997)

---

**C√≥digo del proyecto**: Disponible en [GitHub Repository](#)

¬øHas implementado compresi√≥n de contexto en tus sistemas RAG? Comparte tus experiencias en los comentarios.

